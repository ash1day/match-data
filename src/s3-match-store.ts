import { execSync } from 'child_process'
import * as fs from 'fs'
import * as path from 'path'
import * as zlib from 'zlib'
import { Database } from 'duckdb-async'
import type { MatchTFTDTO } from 'twisted/dist/models-dto'

/**
 * S3ãƒ™ãƒ¼ã‚¹ã®ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆã‚¢
 *
 * ãƒ•ãƒ­ãƒ¼:
 * 1. S3ã‹ã‚‰æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
 * 2. æ—¢å­˜ã®ãƒãƒƒãƒIDã‚’èª­ã¿è¾¼ã¿
 * 3. æ–°è¦ãƒãƒƒãƒã®ã¿APIå–å¾—
 * 4. ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸ã—ã¦S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
 */

const DATA_DIR = process.cwd()

/**
 * S3ã‹ã‚‰æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
 */
export async function downloadFromS3(): Promise<void> {
  console.log('ğŸ“¥ Downloading existing data from S3...')
  try {
    execSync('tsx src/sync-s3.ts download', { stdio: 'inherit', cwd: DATA_DIR })
    console.log('âœ… Download complete')
  } catch (error) {
    console.warn('âš ï¸ Failed to download from S3 (may be first run):', error)
  }
}

/**
 * S3ã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
 */
export async function uploadToS3(): Promise<void> {
  console.log('ğŸ“¤ Uploading data to S3...')
  execSync('tsx src/sync-s3.ts upload', { stdio: 'inherit', cwd: DATA_DIR })
  console.log('âœ… Upload complete')
}

/**
 * æ—¢å­˜ã®ãƒãƒƒãƒIDã‚’å–å¾—
 */
export async function getExistingMatchIds(region: string, patch: string): Promise<Set<string>> {
  const indexPath = path.join(DATA_DIR, region, patch, 'index.json.gz')

  if (!fs.existsSync(indexPath)) {
    console.log(`  No existing index for ${region}/${patch}`)
    return new Set()
  }

  const compressed = fs.readFileSync(indexPath)
  const decompressed = zlib.gunzipSync(compressed)
  const matchIds = JSON.parse(decompressed.toString()) as string[]

  console.log(`  Found ${matchIds.length} existing matches in ${region}/${patch}`)
  return new Set(matchIds)
}

/**
 * æ–°è¦ãƒãƒƒãƒIDã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
 */
export async function filterNewMatchIds(matchIds: string[], region: string, patch: string): Promise<string[]> {
  const existingIds = await getExistingMatchIds(region, patch)
  const newIds = matchIds.filter((id) => !existingIds.has(id))

  console.log(`  ${newIds.length} new matches to fetch (${existingIds.size} already exist)`)
  return newIds
}

/**
 * ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆParquetå½¢å¼ï¼‰
 */
export async function saveMatchData(matches: MatchTFTDTO[], region: string, patch: string): Promise<void> {
  const dir = path.join(DATA_DIR, region, patch)
  fs.mkdirSync(dir, { recursive: true })

  const parquetPath = path.join(dir, 'matches.parquet')
  const tempParquetPath = path.join(dir, 'matches_new.parquet')

  // æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯ãƒãƒ¼ã‚¸
  let existingMatches: MatchTFTDTO[] = []
  if (fs.existsSync(parquetPath)) {
    console.log(`  Loading existing matches from ${parquetPath}...`)
    const db = await Database.create(':memory:')
    await db.run(`INSTALL parquet; LOAD parquet;`)

    const result = await db.all(`SELECT * FROM parquet_scan('${parquetPath}')`)
    existingMatches = result as MatchTFTDTO[]
    console.log(`  Loaded ${existingMatches.length} existing matches`)
    await db.close()
  }

  // ãƒãƒƒãƒIDã§ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–
  const matchMap = new Map<string, MatchTFTDTO>()
  for (const match of existingMatches) {
    matchMap.set(match.metadata.match_id, match)
  }
  for (const match of matches) {
    matchMap.set(match.metadata.match_id, match)
  }

  const allMatches = Array.from(matchMap.values())
  console.log(`  Saving ${allMatches.length} total matches to ${tempParquetPath}`)

  // DuckDBã§Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
  const db = await Database.create(':memory:')
  await db.run(`INSTALL parquet; LOAD parquet;`)

  // JSONã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã«èª­ã¿è¾¼ã¿
  const jsonPath = path.join(dir, 'temp_matches.json')
  // BigIntã‚’æ–‡å­—åˆ—ã«å¤‰æ›ã—ã¦ã‹ã‚‰JSONåŒ–
  fs.writeFileSync(
    jsonPath,
    JSON.stringify(allMatches, (key, value) => (typeof value === 'bigint' ? value.toString() : value))
  )

  // read_json with explicit JSON type to preserve structure
  await db.run(`
    CREATE TABLE matches AS 
    SELECT 
      metadata,
      info
    FROM read_json('${jsonPath}', 
      columns = {
        metadata: 'JSON',
        info: 'JSON'
      }
    )
  `)

  // Parquetã¨ã—ã¦ä¿å­˜
  await db.run(`COPY matches TO '${tempParquetPath}' (FORMAT PARQUET, COMPRESSION ZSTD)`)

  await db.close()

  // ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
  fs.unlinkSync(jsonPath)

  // æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã§ç½®ãæ›ãˆ
  if (fs.existsSync(parquetPath)) {
    fs.unlinkSync(parquetPath)
  }
  fs.renameSync(tempParquetPath, parquetPath)

  console.log(`  âœ… Saved ${allMatches.length} matches to ${parquetPath}`)
}

/**
 * ãƒãƒƒãƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜
 */
export async function saveMatchIndex(matchIds: string[], region: string, patch: string): Promise<void> {
  const dir = path.join(DATA_DIR, region, patch)
  fs.mkdirSync(dir, { recursive: true })

  const indexPath = path.join(dir, 'index.json.gz')

  // æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿
  let existingIds: string[] = []
  if (fs.existsSync(indexPath)) {
    const compressed = fs.readFileSync(indexPath)
    const decompressed = zlib.gunzipSync(compressed)
    existingIds = JSON.parse(decompressed.toString()) as string[]
  }

  // ãƒãƒ¼ã‚¸ã—ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–
  const allIds = Array.from(new Set([...existingIds, ...matchIds]))

  // åœ§ç¸®ã—ã¦ä¿å­˜
  const compressed = zlib.gzipSync(
    JSON.stringify(allIds, (key, value) => (typeof value === 'bigint' ? value.toString() : value))
  )
  fs.writeFileSync(indexPath, compressed)

  console.log(`  âœ… Saved ${allIds.length} match IDs to index`)
}

/**
 * ãƒ‡ãƒ¼ã‚¿åé›†ãƒ—ãƒ­ã‚»ã‚¹ã®åˆæœŸåŒ–
 */
export async function initDataStore(): Promise<void> {
  // S3ã‹ã‚‰æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
  await downloadFromS3()
}

/**
 * ãƒ‡ãƒ¼ã‚¿åé›†ãƒ—ãƒ­ã‚»ã‚¹ã®å®Œäº†
 */
export async function finalizeDataStore(): Promise<void> {
  // S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
  await uploadToS3()
}

/**
 * ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
 */
export async function loadPlayerData(region: string): Promise<any> {
  const filePath = path.join(DATA_DIR, region, 'players.json.gz')

  if (!fs.existsSync(filePath)) {
    return null
  }

  const compressed = fs.readFileSync(filePath)
  const decompressed = zlib.gunzipSync(compressed)
  return JSON.parse(decompressed.toString())
}

/**
 * ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
 */
export async function savePlayerData(players: any[], region: string): Promise<void> {
  const dir = path.join(DATA_DIR, region)
  fs.mkdirSync(dir, { recursive: true })

  const filePath = path.join(dir, 'players.json.gz')
  const compressed = zlib.gzipSync(
    JSON.stringify(players, (key, value) => (typeof value === 'bigint' ? value.toString() : value))
  )
  fs.writeFileSync(filePath, compressed)

  console.log(`  âœ… Saved ${players.length} players to ${filePath}`)
}
